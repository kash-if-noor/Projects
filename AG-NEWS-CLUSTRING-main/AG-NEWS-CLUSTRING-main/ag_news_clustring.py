# -*- coding: utf-8 -*-
"""AG NEWS CLUSTRING

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lJNcb57Q50-fwjEES9RR-TvIx4qXVEdC
"""

import pandas as pd
df_train=pd.read_csv('/content/train.csv')
df_test=pd.read_csv('/content/test.csv')
df_train.head()

df_test.head()

df_train=df_train.drop(['Class Index'],axis=1)
df_test=df_test.drop(['Class Index'],axis=1)

df_train.head()

df_train["text"] = df_train["Title"] + " " + df_train["Description"]
df_test["text"] = df_test["Title"] + " " + df_test["Description"]
df_train = df_train[["text"]]     # keep only the combined text column
df_test=df_test[["text"]]

import re
import nltk
from nltk.corpus import stopwords

# Download stopwords (only once)
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# Cleaning function
def clean_text(t):
    t = t.lower()                          # lowercase
    t = re.sub(r"[^a-zA-Z\s]", "", t)      # remove special chars & numbers
    t = re.sub(r"\s+", " ", t).strip()     # remove extra spaces
    t = " ".join([word for word in t.split() if word not in stop_words])  # remove stopwords
    return t
df_train["clean_text"] = df_train["text"].apply(clean_text)
df_test["clean_text"] = df_test["text"].apply(clean_text)

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize
vectorizer = TfidfVectorizer(max_features=5000)

# Fit on TRAIN and transform TRAIN
X_train = vectorizer.fit_transform(df_train["clean_text"])

# Transform TEST using the same vectorizer
X_test = vectorizer.transform(df_test["clean_text"])

# ==============================
# AG News Clustering - TF-IDF + KMeans
# ==============================
from sklearn.cluster import KMeans

# 4️⃣ KMeans Clustering
k = 4  # number of clusters
kmeans = KMeans(n_clusters=k, random_state=42)
df_train["cluster"] = kmeans.fit_predict(X_train)
df_test["cluster"] = kmeans.predict(X_test)

# 5️⃣ Inspect clusters
for i in range(k):
    print(f"\n--- Cluster {i} ---")
    print(df_train[df_train["cluster"] == i]["text"].head(3).to_list())

print(df_train["cluster"].value_counts())

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_train.toarray())

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=df_train["cluster"], cmap='viridis', alpha=0.6)
plt.title("AG News Clusters (PCA 2D)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

import numpy as np

# Get feature names from the TF-IDF vectorizer
feature_names = np.array(vectorizer.get_feature_names_out())

# Number of top words to show
top_n = 10

print("Top terms per cluster:\n")
for i in range(k):  # k = 4
    # Get the cluster center for cluster i
    cluster_center = kmeans.cluster_centers_[i]

    # Sort words by importance in descending order
    top_features_idx = cluster_center.argsort()[::-1][:top_n]
    top_terms = feature_names[top_features_idx]

    print(f"Cluster {i} top terms: {', '.join(top_terms)}")

cluster_labels = {
    0: "Sports",
    1: "World",
    2: "Business",
    3: "Markets"
}

df_train["topic"] = df_train["cluster"].map(cluster_labels)
df_test["topic"] = df_test["cluster"].map(cluster_labels)

print(df_train[["text", "cluster", "topic"]].head())

# Example new article
new_article = "Microsoft launches new AI-powered game console next month"

# 1️⃣ Clean the text
def clean_text(t):
    t = t.lower()
    t = re.sub(r"[^a-zA-Z\s]", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    t = " ".join([word for word in t.split() if word not in stop_words])
    return t

cleaned_article = clean_text(new_article)

# 2️⃣ Convert to TF-IDF
X_new = vectorizer.transform([cleaned_article])

# 3️⃣ Predict cluster
pred_cluster = kmeans.predict(X_new)[0]

# 4️⃣ Map to topic
pred_topic = cluster_labels[pred_cluster]

print(f"Predicted cluster: {pred_cluster}")
print(f"Predicted topic: {pred_topic}")